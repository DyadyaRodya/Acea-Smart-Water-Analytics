---
title: "Ekz"
author: "Yulian Volianskiy"
date: "06 01 2021"
output: html_document
---

1. Reading data and transform Date from char to Date format

```{r}
library(lubridate)
library(ggplot2)

setwd("E:/prazia/univer/DataScience/Ekz/acea-water-prediction")
df <- read.csv("Water_Spring_Lupa.csv")
df$п.їDate = strptime(df$п.їDate, "%d/%m/%Y")
df$Date <- ymd(df$п.їDate)
ggplot(data=df, aes(x=Date, y=Flow_Rate_Lupa)) + geom_line()
head(df)

df_data <- read.csv("water_spring_lupa_csv_r2.csv")
head(df_data)
df_train <- read.csv("water_spring_lupa_csv_r.csv")
head(df_data)

library(Amelia)
missmap(df_train, main = "Missing values vs observed")
missmap(df_data, main = "Missing values vs observed")

train1 <- head(df_train, 3000)
test1 <- tail(df_train, 817)

```
Here we can see a plot, that shows us an empty parts in our data. So first of all we should to find Nans and predict them. I divided data in two types:
1. Data with Nans
2. Data without empty fields

After that I divided data without empty fields into train and test datas. It will be useful for linear regresion model. 


Next step - I should create a logistic regresion using train data.
```{r}

library(ggplot2)
library(hexbin)

ggplot(df_train, aes(x = flow_rate_lupa, y = rainfall_terni)) + geom_point(alpha = 0.1)
hexbinplot(rainfall_terni ~ flow_rate_lupa, data=df_train)

df_trainLM = lm(flow_rate_lupa ~ rainfall_terni, data = df_train) #Create the linear regression
summary(df_trainLM)

```
If we'll inspect coeficients, we'll see that this model isn't strong due to:
R-squared - it's apeared to be too small, near zero.
p_value - not too small.
That can say us, that we need probably more data to culculate stronger linear regresion model.


Using prediction function we try to predict values in empty fields
```{r}

newdata1 <- test1$flow_rate_lupa
df_data$flow_rate_lupa <-predict(df_trainLM,newdata=df_data,type="response")
df_new <- rbind(df_data,df_train)

df_new$п.їDate = strptime(df_new$п.їDate, "%d/%m/%Y")
df_new$Date <- ymd(df_new$п.їDate)

ggplot(data=df_new, aes(x=Date, y=df_new$flow_rate_lupa)) + geom_line()

```
In the plot we can see that it hasn't even distribution, so that kind of prediction data doesn't fit.



Another way to predict data - using interpolation function, so lets try it:
```{r}
library(imputeTS)
library(lubridate)
df$Flow_Rate_Lupa <- na.interpolation(df$Flow_Rate_Lupa, option = "spline")


df$Date <- ymd(df$п.їDate)

library(ggplot2)

ggplot(data=df, aes(x=Date, y=Flow_Rate_Lupa)) + geom_line()

#plot(x=df$п.їDate, y=df$Flow_Rate_Lupa)

```
In the plot we can see, that it probably fits our problem and have even distribution in empty places. 

Prepare data and sort by date. Check timeinterval.

```{r}

library(data.table)

df <- df[order(df$Date), ]
df['Time_Interval'] = df$Date - shift(df$Date, n=1, fill=NA, type="lag")

days <- df$Time_Interval


for(i in days)
{
  
  if(isTRUE(i > 1) || is.null(i))
  {
    print(i)
  }
    
}

```




```{r}
myts <- ts(df$Flow_Rate_Lupa, start=c(2009, 1, 1), end=c(2020, 30, 6), frequency=52)
```


#Augmented Dickey-Fuller (ADF)

If the null hypothesis can be rejected, we can conclude that the time series is stationary.

There are two ways to rejects the null hypothesis:

On the one hand, the null hypothesis can be rejected if the p-value is below a set significance level. The defaults significance level is 5%

**p-value > significance level (default: 0.05)**: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.
**p-value <= significance level (default: 0.05)**: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.
On the other hand, the null hypothesis can be rejects if the test statistic is less than the critical value.

```{r}
library(tseries)
result4 = adf.test(myts)
result4
```
As we can see from previous test, p_value > 0.05, so we should do transformation and differencing:

```{r}

#myts

jjDataNewLog <- log10(abs(myts) + 1)
#jjDataNewLogDiff <- diff(jjDataNewLog)
#jjDataNewLogDiff <- na_interpolation(jjDataNewLogDiff, option = "linear")
plot(jjDataNewLog)

```
After this, try again a test:
```{r}
result4 = adf.test(jjDataNewLog)
result4
```
```{r}

jjDataNewLog <- sqrt(jjDataNewLog)
result4 = adf.test(jjDataNewLog)
result4

```



We see that p_value is normal, so let's use this data

```{r}
decomp = decompose(jjDataNewLog, type='additive')

plot(decomp$trend)
plot(decomp$seasonal)

```

Split our data in train and test and try acf and pacf test for our data:

```{r}
library(TSstudio)

split_USgas <- ts_split(ts.obj = jjDataNewLog, 130)

plot(split_USgas$train)
plot(split_USgas$test)

acf(jjDataNewLog)
pacf(jjDataNewLog)
```

Using coefficients (1,1,8) for ARIMA we will test it.

```{r}
library(forecast)

auto.arima(split_USgas$train, trace=TRUE) 

fit <- arima(split_USgas$train, order = c(1,1,8))
print(paste('ARIMA(1,1,8) - AICc: ', round(fit$aic,2)))
fit.test1 <- arima(split_USgas$train, order = c(1,1,9))
print(paste('ARIMA(1,1,9) - AICc: ', round(fit.test1$aic,2)))
fit.test2 <- arima(split_USgas$train, order = c(1,1,10))
print(paste('ARIMA(1,1,10) - AICc: ', round(fit.test2$aic,2)))
fit.test3 <- arima(split_USgas$train, order = c(0,1,8))
print(paste('ARIMA(0,1,8) - AICc: ', round(fit.test3$aic,2)))
fit.test4 <- arima(split_USgas$train, order = c(0,1,9))
print(paste('ARIMA(0,1,9) - AICc: ', round(fit.test4$aic,2)))

```
After testing, we understood that fit.test3 is good for this data

```{r}

checkresiduals(fit.test3)
tsdisplay(residuals(fit.test3), lag.max = 20)

```

try to predict data, using our model

```{r}

newdata2 <-forecast(fit.test3, h = 390, type="response")
plot(split_USgas$train)
plot(newdata2)


clrs <- c("blue", "red")
autoplot(split_USgas$train) +
  autolayer(newdata2,
    series="Forecast", PI=FALSE) +
  autolayer(split_USgas$test,
    series="Test", PI=FALSE) +
  ggtitle("Forecastsing") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))+
      scale_color_manual(values=clrs)

autoplot(newdata2) +
  autolayer(split_USgas$test,
    series="Test", PI=FALSE) +
  ggtitle("Forecastsing") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))+
      scale_color_manual(values=clrs)


```

try to predict data, using forecast methods

```{r}

library(TSstudio)
library(forecast)
split_USgas <- ts_split(ts.obj = myts, 130)

plot(split_USgas$train)
plot(split_USgas$test)


autoplot(split_USgas$train) +
  autolayer(meanf(split_USgas$train),
    series="Mean", PI=FALSE) +
  autolayer(naive(split_USgas$train),
    series="Naïve", PI=FALSE) +
  autolayer(snaive(split_USgas$train),
    series="Seasonal naïve", PI=FALSE) +
   autolayer(split_USgas$test,
    series="Test", PI=FALSE)+
  ggtitle("Forecasts for quarterly beer production") +
  xlab("Year") + ylab("Megalitres") +
  guides(colour=guide_legend(title="Forecast"))

```
So our model shows us better results, than forecast methods


In the end we are doing a crossvalidation:
```{r}

#Fit an AR(2) model to each rolling origin subset
far2 <- function(x, h){forecast(fit.test4, h=h)}
#e <- tsCV(split_USgas$train, far2, h=1)
#plot(e)

#Fit the same model with a rolling window of length 30
e <- tsCV(split_USgas$train, far2, h=1)
e1 <- tsCV(split_USgas$test, far2, h=1)
autoplot(e) + autolayer(split_USgas$test) + autolayer(split_USgas$train) + autolayer(e1)
```












